Lock ahead design
Lock ahead is a proposed new Lustre feature aimed at solving a long standing problem with shared file write performance in Lustre.  It requires client and server support, and is targeted as a possible feature for Lustre 2.8 upstream, and for Cray Lustre 2.7.  It will be used primarily via the MPI-I/O library.
The first part of this document (sections 1 and 2) is an overview of the problem and high level description of the solution.  Section 3 explains how the library will make use of this feature, and sections 4 and 5 describe the design of the Lustre changes.  Sections 4 and 5 are, in some sense, a guide to reviewing the code for this feature.
1. Overview: Purpose & Interface
Lock ahead is intended to allow optimization of certain I/O patterns which would otherwise suffer LDLM* lock contention. Lock ahead allows applications which know their I/O pattern use that information to avoid locking problems in LDLM.
*Lustre distributed lock manager.  This is the locking layer shared between clients and servers, to manage access between clients.
Normally, clients request locks as the first step of an I/O.  The client asks for a lock which covers exactly the area of interest (IE, a read or write lock of n bytes at offset x), but the server attempts to optimize this by expanding the lock to cover as much of the file as possible.  This is useful for a single client, but can be trouble for multiple clients.
In cases where multiple clients wish to write to the same file, this optimization can result in locks that conflict when the actual I/O operations do not.  This requires clients to wait for one another to complete I/O, even when there is no conflict between actual I/O requests.  This can significantly reduce performance (Anywhere from 40-90%, depending on system specs) for some workloads.
Lock ahead avoids this problem by acquiring the necessary locks in advance, by explicit requests with server side optimization disabled.  For this, we add a new ioctl which allows clients to request a lock, specifying the extent and the I/O mode (read/write) for the lock.  These lock requests explicitly disable server side optimization, so the lock returned to the client covers only the requested extent.
When using lock ahead, clients which intend to write to a file request locks to cover their I/O pattern, wait a moment for the locks to be granted, then begin writing to the file.
In this way, a set of clients which knows their I/O pattern in advance can force the LDLM layer to grant locks appropriate for that I/O pattern.  This allows applications which are poorly handled by the default  lock optimization behavior to significantly improve their performance.
2. I/O Pattern & Locking problems
2. A. Strided writing and MPI-I/O
There is an excellent explanation and overview of strided writing and the benefits of lock ahead in the slides from my LUG presentation this year.  It is highly recommended to read that first, as the graphics are much clearer than the prose here.
See slides 1-13:
http://cdn.opensfs.org/wp-content/uploads/2015/04/Shared-File-Performance-in-Lustre_Farrell.pdf
MPI-I/O uses strided writing when doing I/O from a large job to a single file.  I/O is aggregated from all the nodes running a particular application to a small number of I/O aggregator nodes which then write out the data, in a strided manner.
In strided writing, different clients take turns writing different blocks of a file (A block is some arbitrary number of bytes).  Client 1 is responsible for writes to block 0, block 2, block 4, etc., client 2 is responsible for block 1, block 3, etc.  Without lock ahead, strided writing is set up in concert with Lustre file striping so each client writes to one OST.  (IE, for a file striped to three OSTs, we would use three clients.)
The particular case of interest is when we want to use more than one client per OST.  This is important, because an OST typically has much more bandwidth than one client.  Strided writes are non-overlapping, so they should be able to proceed in parallel with more than one client per OST.  In practice, on Lustre, they do not, due to lock expansion.
2. B. Locking problems
We will now describe locking when there is more than one client per OST.  This behavior is the same on a per OST basis in a file striped across multiple OSTs.
When the first client asks to write block 0, it requests the required lock from the server.  When it receives this request, the server sees that there are no other locks on the file.  Since it assumes the client will want to write to the file again, the server expands the lock as far as possible.  In this case, it expands the lock to the maximum file size (effectively, to infinity), then grants it to client 1.
When client 2 wants to write block 1, it conflicts with the expanded lock granted to client 1.  The server then must revoke (In Lustre terms, 'call back') the lock granted to client 1 so it can grant a lock to client 2.  After the lock granted to client 1is revoked, there are no locks on the file.  The server sees this when processing the lock request from client 2, and expands that lock to cover the whole file.
Client 1 then wishes to write block 3 of the file...  And the cycle continues.  The two clients exchange the extended lock throughout the write, allowing only one client to write at a time, plus latency to exchange the lock.  The effect is dramatic: Two clients are actually slower than one.  (Similar behavior is seen with more than two clients.)
The solution is to use lock ahead to request locks before they are needed.  In effect, before it starts writing to the file, client 1 requests locks on block 0, block 2, etc – It locks 'ahead' a certain (tunable) number of locks.  Client 2 does the same.  Then they both begin to write, and are able to do so in parallel.  A description of the actual library implementation follows.
3. Library implementation
Actually implementing this in the library carries a number of wrinkles.  The basic pattern is this:
Before writing, an I/O aggregator requests a certain number of locks on blocks that it is responsible for.  It may or may not ever write to these blocks, but it takes locks knowing it might.  It then begins to write, tracking how many of the locks it has used.  When the number of locks 'ahead' of the I/O is low enough, it requests more locks in advance of the I/O.  
For technical reasons which are explained in the implementation section, lock ahead lock requests are non-blocking.  In Lustre terms, this means if there is already a lock on the relevant extent of the file, the lock ahead request is not granted.  This means that if there is already a lock on the file (not uncommon – Imagine writing to a file which was previously read by another process), lock ahead will not work without something else.
For that, we use Lustre's group lock functionality.  A group lock is a special type of exclusive, full file lock that a client can request with an ioctl.  Group locks have a number of features we'll ignore here – The key features are that they can be explicitly requested and cancelled by a client, and they cause all other locks on a file to be cancelled.  So the library has a singe client take and then release a group lock before it starts writing to a file.  This 'cleans' the file so lock ahead can proceed normally.  
It is of course possible for another process to get in the way by immediately requesting a lock on the file.  To this, the primary response is 'Don't.'.  When writing out a file, repeatedly trying to read it will impact performance even without lock ahead.  However, we do have a way of mitigating the impact of interfering locks which would otherwise prevent lock ahead.
These interfering locks can also happen if a lock ahead lock is, for some reason, not available in time for the write request which intended to use it.  The lock which results from this write request is expanded using the normal rules.  So it's possible for that lock (depending on the position of other locks at the time) to be extended to cover the rest of the file.  That will block future lock ahead requests.
The expanded lock will be revoked when a write request happens in the range covered by that lock, but the lock for that request will be expanded as well - And we return to handing the lock back and forth between clients.  These expanded locks will still block future lock ahead requests.  So lock ahead becomes useless.
The way to avoid this is to turn off lock expansion for I/O requests which are supposed to be using lock ahead locks.  That way, if a lock ahead lock is not available, the lock request for the I/O will not be expanded.  Instead, that request will cancel any interfering locks, but the lock for that request will not be expanded.  This leaves the later parts of the file open, allowing future lock ahead requests to succeed.  This means that if an interfering lock blocks some lock ahead requests, those are lost, but the next set of lock ahead requests can proceed as normal.
In effect, lock ahead is interrupted, but then is able to re-assert itself.  The feature used here is referred to as 'request only' locking (as the only extent used by the actual I/O request is locked) and is turned on with an ioctl.  The library will do this on the file descriptor it uses for lock ahead and writing.

4. Client side design
4. A. Lock ahead
Lock ahead uses the existing asynchronous lock request functionality implemented for asynchronous glimpse locks (AGLs), a long standing Lustre feature.  AGLs are locks which are requested by statahead, which are used to get file size information before it's requested.  The key thing about an asynchronous lock request is that it does not have a specific I/O request waiting to use the lock.  This means two key things:

1. There is no OSC lock (lock layer above LDLM for data locking) associated with the LDLM lock
2. There is no thread waiting for the LDLM lock reply, so the reply must be handled by the networking daemon thread (PTLRPCD)
Since both of these issues are addressed by the asynchronous lock request code which lock ahead shares with AGL, we will not explore them in depth here.
Finally, lock ahead requests set the CEF_REQ_ONLY flag, which tells the OSC (per OST client layer) layer of Lustre to set LDLM_FL_NO_EXPANSION on any lock requests.  LDLM_FL_NO_EXPANSION is a new LDLM lock flag which tells the server not to expand the requested lock.
This leaves the user facing interface.  Lock ahead is implemented as an ioctl, which takes a mode, a flags argument, a version code (to allow future API changes without breakage), a count of extents, and an array of extents.
Lock ahead will then make lock requests on these extents, one after another.  Because the lock requests are asynchronous (replies are handled by ptlrpcd), many requests can be made quickly.
4. B. Request only Ioctl
The request only ioctl uses the ioctl interface to set a boolean in a Lustre data structure associated with a file descriptor (ll_file_data).  This is then used to set a corresponding boolean in all cl_io structs associated with this file descriptor (it is also passed on to associated sub_io structs).  In the vvp layer of Lustre (One of the upper layers, which helps translate POSIX I/O requests to things Lustre understands.), this bool is used to set the same CEF_REQ_ONLY flag used by lock ahead.  Just like for lock ahead, this sets LDLM_FL_NO_EXPANSION in all requests sent to the server.

5. Server side changes
Implementing lock ahead requires server support for LDLM_FL_NO_EXPANSION, but it also required an additional pair of server side changes to fix issues which came up because of lock ahead.  These changes are not part of the core lock ahead design, instead, they are separate fixes which are required for it to work.

5. A. Support LDLM_FL_NO_EXPANSION

Disabling server side lock expansion is done with a new LDLM flag.  This is done with a simple check for that flag on the server before attempting to expand the lock.  If the flag is found, lock expansion is skipped.

5. B. Fully implement BLOCK_NOWAIT

As described above, lock ahead locks are non-blocking.  This means they use the BLOCK_NOWAIT LDLM flag.  At first, setting BLOCK_NOWAIT had no effect.  Examining the code showed that the BLOCK_NOWAIT flag was only implemented for group locks, not general extent locks (it did nothing).

Fixing this required a few new lines of code in ldlm_extent_compat_queue to extend the BLOCK_NOWAIT behavior, mirroring the behavior for group locks.

5. C. File size – ofd_intent_policy changes

Knowing the current file size during writes is tricky on a distributed file system, because multiple clients can be writing to a file at any time.  When writes are in progress, the server must identify which client is currently responsible for growing the file size, and ask that client what the file size is.

To do this, the server uses glimpse locking (in ofd_intent_policy) to get the current file size from the clients.  This code uses the assumption that the holder of the highest write lock (PW lock) knows the current file size.  A client learns the (then current) file size when a lock is granted.  Because only the holder of the highest lock can grow a file, either the size hasn't changed, or that client knows the new size; so the server only has to contact the client which holds this lock, and it knows the current file size.

Note that the above is actually racy – When the server asks, the client can still be writing, or another client could acquire a higher lock during this time.  The goal is a good approximation while the file is being written, and a correct answer once all the clients are done writing.  This is achieved because once writes to a file are complete, the holder of that highest lock is guaranteed to know the current file size.  This is where lock ahead causes trouble.

By creating write locks in advance of an actual I/O, lock ahead breaks the assumption that the holder of the highest lock knows the file size.  Normally, locks are taken as part of an I/O request.  Lock ahead locks are not, and so, the holder of a write lock does not necessarily write to the file in the area covered by that lock.

Consider:  Two clients, A and B, strided writing.  Each client requests, for example, 2 lock ahead locks.  (Real numbers are much higher.)  Client A holds locks on segments 0 and 2, client B holds locks on segments 1 and 3.

The request comes to write 3 segments of data.  Client A writes to segment 0, client B writes to segment 1, and client A also writes to segment 2.  No data is written to segment 3.  At this point, the server checks the file size, by glimpsing the highest lock – The lock on segment 3.  Client B does not know about the writing done by client A to segment 2, so it gives an incorrect file size.

This would be OK if client B had pending writes to segment 3, but it does not.  In this situation, the server will never get the correct file size while this lock exists.

The solution is relatively straightforward: The server needs to ask glimpse every write lock to check the current file size is, then take the largest size returned.  This avoids asking only a client which may not know the correct file size, by making sure to ask all the clients.  (We iterate over locks because it is actually locks which know the file size, not clients.)

The code is modified accordingly, iterating through the write locks on a file and glimpsing each one.

While it is not ideal to glimpse all the write locks on a file instead of one, the cost is mitigated by a few factors:

Since write locks are exclusive (and by default, are expanded to the largest possible size), it's very rare for there to be more than a few write locks on a file.

The only situation which creates a large number of write locks is actually lock ahead, and we tune carefully to not request too many locks in advance of need.

Also, lock cancellation methods such as early lock cancel aggressively clean up older locks.

In the end, the final verdict here is performance – Lock ahead testing has shown good performance results despite this weak point.
