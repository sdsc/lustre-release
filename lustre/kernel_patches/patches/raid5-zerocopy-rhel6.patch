diff --git a/drivers/md/raid5.c b/drivers/md/raid5.c
index 9cd137e..6d1a32d 100644
Index: linux-2.6.32-131.12.1.el6.x86_64/drivers/md/raid5.c
===================================================================
--- linux-2.6.32-131.12.1.el6.x86_64.orig/drivers/md/raid5.c	2011-09-23 14:49:20.000000000 +0300
+++ linux-2.6.32-131.12.1.el6.x86_64/drivers/md/raid5.c	2011-09-23 14:49:54.000000000 +0300
@@ -193,5 +193,24 @@ static int stripe_operations_active(stru
 }
 
+static struct page *zero_copy_data(struct bio *bio, sector_t sector)
+{
+	sector_t bi_sector = bio->bi_sector;
+	struct page *page = NULL;
+	struct bio_vec *bvl;
+	int i;
+
+	bio_for_each_segment(bvl, bio, i) {
+		if (sector == bi_sector && bvl->bv_len == STRIPE_SIZE) {
+			page = bvl->bv_page;
+			if (PageConstant(page))
+				return page;
+			return NULL;
+		}
+		bi_sector += bvl->bv_len >> 9;
+	}
+	return NULL;
+}
+
 static void __release_stripe(struct r5conf *conf, struct stripe_head *sh)
 {
 	if (atomic_dec_and_test(&sh->count)) {
@@ -495,6 +514,26 @@ raid5_end_read_request(struct bio *bi, i
 static void
 raid5_end_write_request(struct bio *bi, int error);
 
+static inline void r5dev_switch_page(struct r5dev *dev, struct page *page)
+{
+	BUG_ON(dev->page_save != NULL);
+	BUG_ON(dev->page != bio_iovec_idx(&dev->req, 0)->bv_page);
+	/* The pointer must be restored whenever the LOCKED gets cleared. */
+	dev->page_save = dev->page;
+	dev->page = bio_iovec_idx(&dev->req, 0)->bv_page = page;
+	kmap(dev->page); /* for sync_xor on 32-bit systems */
+}
+
+static inline void r5dev_restore_page(struct r5dev *dev)
+{
+	BUG_ON(dev->page_save == NULL);
+	BUG_ON(dev->page != bio_iovec_idx(&dev->req, 0)->bv_page);
+	BUG_ON(dev->page == dev->page_save);
+	kunmap(dev->page_save);
+	dev->page = bio_iovec_idx(&dev->req, 0)->bv_page = dev->page_save;
+	dev->page_save = NULL;
+}
+
 static void ops_run_io(struct stripe_head *sh, struct stripe_head_state *s)
 {
 	raid5_conf_t *conf = sh->raid_conf;
@@ -563,6 +602,10 @@ static void ops_run_io(struct stripe_hea
 				set_bit(STRIPE_DEGRADED, &sh->state);
 			pr_debug("skip op %ld on disc %d for sector %llu\n",
 				bi->bi_rw, i, (unsigned long long)sh->sector);
+
+			if (test_bit(R5_Direct, &sh->dev[i].flags))
+				r5dev_restore_page(&sh->dev[i]);
+
 			clear_bit(R5_LOCKED, &sh->dev[i].flags);
 			set_bit(STRIPE_HANDLE, &sh->state);
 		}
@@ -1009,6 +1052,33 @@ ops_run_prexor(struct stripe_head *sh, s
 	return tx;
 }
 
+static int try_reuse_data_page(struct r5dev *dev)
+{
+	struct bio *wbi = dev->written;
+	struct page *page;
+	sector_t sector = dev->sector;
+
+	BUG_ON(!test_bit(R5_LOCKED, &dev->flags));
+	BUG_ON(test_bit(R5_Direct, &dev->flags));
+
+	/* check if it's covered by a single page
+	   and the whole stripe is written at once.
+	 * in this case we can avoid memcpy() */
+	if (wbi && !wbi->bi_next && test_bit(R5_OVERWRITE, &dev->flags) &&
+	    test_bit(R5_Insync, &dev->flags)) {
+		page = zero_copy_data(wbi, sector);
+		if (page) {
+			set_bit(R5_Direct, &dev->flags);
+			r5dev_switch_page(dev, page);
+			clear_bit(R5_UPTODATE, &dev->flags);
+			clear_bit(R5_OVERWRITE, &dev->flags);
+			return 1;
+		}
+	}
+
+	return 0;
+}
+
 static struct dma_async_tx_descriptor *
 ops_run_biodrain(struct stripe_head *sh, struct dma_async_tx_descriptor *tx)
 {
@@ -1032,6 +1102,13 @@ ops_run_biodrain(struct stripe_head *sh,
 			wbi = dev->written = chosen;
 			spin_unlock(&sh->lock);
 
+			if (try_reuse_data_page(dev)) {
+				if (wbi->bi_rw & REQ_FUA)
+					set_bit(R5_WantFUA, &dev->flags);
+				atomic_inc(&sh->raid_conf->writes_zcopy);
+				continue;
+			}
+
 			while (wbi && wbi->bi_sector <
 				dev->sector + STRIPE_SECTORS) {
 				if (wbi->bi_rw & REQ_FUA)
@@ -1065,7 +1142,8 @@ static void ops_complete_reconstruct(voi
 		struct r5dev *dev = &sh->dev[i];
 
 		if (dev->written || i == pd_idx || i == qd_idx) {
-			set_bit(R5_UPTODATE, &dev->flags);
+			if (!test_bit(R5_Direct, &dev->flags))
+				set_bit(R5_UPTODATE, &dev->flags);
 			if (fua)
 				set_bit(R5_WantFUA, &dev->flags);
 		}
@@ -1644,6 +1722,7 @@ static void raid5_end_read_request(struc
 		}
 	}
 	rdev_dec_pending(conf->disks[i].rdev, conf->mddev);
+	BUG_ON(bio_iovec_idx(&sh->dev[i].req, 0)->bv_page != sh->dev[i].page);
 	clear_bit(R5_LOCKED, &sh->dev[i].flags);
 	set_bit(STRIPE_HANDLE, &sh->state);
 	release_stripe(sh);
@@ -1673,6 +1752,8 @@ static void raid5_end_write_request(stru
 
 	rdev_dec_pending(conf->disks[i].rdev, conf->mddev);
 	
+	if (test_bit(R5_Direct, &sh->dev[i].flags))
+		r5dev_restore_page(&sh->dev[i]);
 	clear_bit(R5_LOCKED, &sh->dev[i].flags);
 	set_bit(STRIPE_HANDLE, &sh->state);
 	release_stripe(sh);
@@ -2515,7 +2596,8 @@ static void handle_stripe_clean_event(ra
 		if (sh->dev[i].written) {
 			dev = &sh->dev[i];
 			if (!test_bit(R5_LOCKED, &dev->flags) &&
-				test_bit(R5_UPTODATE, &dev->flags)) {
+				(test_bit(R5_UPTODATE, &dev->flags) ||
+				 test_bit(R5_Direct, &dev->flags))) {
 				/* We can return any write requests */
 				struct bio *wbi, *wbi2;
 				int bitmap_end = 0;
@@ -2523,6 +2605,7 @@ static void handle_stripe_clean_event(ra
 				spin_lock_irq(&conf->device_lock);
 				wbi = dev->written;
 				dev->written = NULL;
+				clear_bit(R5_Direct, &dev->flags);
 				while (wbi && wbi->bi_sector <
 					dev->sector + STRIPE_SECTORS) {
 					wbi2 = r5_next_bio(wbi, dev->sector);
@@ -3225,9 +3225,11 @@ static void handle_stripe(struct stripe_head *sh)
 		/* All the 'written' buffers and the parity block are ready to
 		 * be written back to disk
 		 */
-		BUG_ON(!test_bit(R5_UPTODATE, &sh->dev[sh->pd_idx].flags));
+		BUG_ON(!test_bit(R5_UPTODATE, &sh->dev[sh->pd_idx].flags) &&
+		       !test_bit(R5_Direct, &sh->dev[sh->pd_idx].flags));
 		BUG_ON(sh->qd_idx >= 0 &&
-		       !test_bit(R5_UPTODATE, &sh->dev[sh->qd_idx].flags));
+		       !test_bit(R5_UPTODATE, &sh->dev[sh->qd_idx].flags) &&
+		       !test_bit(R5_Direct, &sh->dev[sh->qd_idx].flags));
 		for (i = disks; i--; ) {
 			struct r5dev *dev = &sh->dev[i];
 			if (test_bit(R5_LOCKED, &dev->flags) &&
@@ -5210,6 +5294,7 @@ static int run(mddev_t *mddev)
 
 		mddev->queue->backing_dev_info.congested_data = mddev;
 		mddev->queue->backing_dev_info.congested_fn = raid5_congested;
+		mddev->queue->backing_dev_info.capabilities |= BDI_CAP_PAGE_CONSTANT_WRITE;
 		mddev->queue->unplug_fn = raid5_unplug_queue;
 
 		chunk_size = mddev->chunk_sectors << 9;
Index: linux-2.6.32-131.12.1.el6.x86_64/drivers/md/raid5.h
===================================================================
--- linux-2.6.32-131.12.1.el6.x86_64.orig/drivers/md/raid5.h	2011-07-31 23:05:58.000000000 +0300
+++ linux-2.6.32-131.12.1.el6.x86_64/drivers/md/raid5.h	2011-09-23 14:49:20.000000000 +0300
@@ -232,7 +232,7 @@ struct stripe_head {
 	struct r5dev {
 		struct bio	req;
 		struct bio_vec	vec;
-		struct page	*page;
+		struct page	*page, *page_save;
 		struct bio	*toread, *read, *towrite, *written;
 		sector_t	sector;			/* sector of this page */
 		unsigned long	flags;
@@ -275,6 +275,7 @@ struct stripe_head_state {
 #define	R5_WantFUA	14	/* Write should be FUA */
 #define	R5_WriteError	15	/* got a write error - need to record it */
 #define	R5_MadeGood	16	/* A bad block has been fixed by writing to it*/
+#define	R5_Direct	31	/* Use the pages in bio to do the write directly. */
 /*
  * Write method
  */
Index: linux-2.6.32-131.12.1.el6.x86_64/include/linux/backing-dev.h
===================================================================
--- linux-2.6.32-131.12.1.el6.x86_64.orig/include/linux/backing-dev.h	2011-07-31 23:06:03.000000000 +0300
+++ linux-2.6.32-131.12.1.el6.x86_64/include/linux/backing-dev.h	2011-09-23 14:49:20.000000000 +0300
@@ -230,6 +230,7 @@ int bdi_set_max_ratio(struct backing_dev
 #define BDI_CAP_EXEC_MAP	0x00000040
 #define BDI_CAP_NO_ACCT_WB	0x00000080
 #define BDI_CAP_SWAP_BACKED	0x00000100
+#define BDI_CAP_PAGE_CONSTANT_WRITE	0x00000200	/* Zcopy write - for raid5 */
 
 #define BDI_CAP_VMFLAGS \
 	(BDI_CAP_READ_MAP | BDI_CAP_WRITE_MAP | BDI_CAP_EXEC_MAP)
@@ -304,6 +305,11 @@ static inline bool bdi_cap_swap_backed(s
 	return bdi->capabilities & BDI_CAP_SWAP_BACKED;
 }
 
+static inline bool bdi_cap_page_constant_write(struct backing_dev_info *bdi)
+{
+	return bdi->capabilities & BDI_CAP_PAGE_CONSTANT_WRITE;
+}
+
 static inline bool bdi_cap_flush_forker(struct backing_dev_info *bdi)
 {
 	return bdi == &default_backing_dev_info;
@@ -324,6 +330,10 @@ static inline bool mapping_cap_swap_back
 	return bdi_cap_swap_backed(mapping->backing_dev_info);
 }
 
+static inline bool mapping_cap_page_constant_write(struct address_space *mapping) {
+	return bdi_cap_page_constant_write(mapping->backing_dev_info);
+}
+
 static inline int bdi_sched_wait(void *word)
 {
 	schedule();
Index: linux-2.6.32-131.12.1.el6.x86_64/include/linux/page-flags.h
===================================================================
--- linux-2.6.32-131.12.1.el6.x86_64.orig/include/linux/page-flags.h	2011-07-31 23:04:50.000000000 +0300
+++ linux-2.6.32-131.12.1.el6.x86_64/include/linux/page-flags.h	2011-09-23 14:49:20.000000000 +0300
@@ -111,6 +111,7 @@ enum pageflags {
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 	PG_compound_lock,
 #endif
+	PG_constant,
 	__NR_PAGEFLAGS,
 
 	/* Filesystems */
@@ -214,6 +215,7 @@ PAGEFLAG(Pinned, pinned) TESTSCFLAG(Pinn
 PAGEFLAG(SavePinned, savepinned);			/* Xen */
 PAGEFLAG(Reserved, reserved) __CLEARPAGEFLAG(Reserved, reserved)
 PAGEFLAG(SwapBacked, swapbacked) __CLEARPAGEFLAG(SwapBacked, swapbacked)
+PAGEFLAG(Constant, constant)
 
 __PAGEFLAG(SlobFree, slob_free)
 
